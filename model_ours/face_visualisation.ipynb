{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d513c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 07:07:35.030729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import imutils\n",
    "from numpy.linalg import inv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from data_read import *\n",
    "from net import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003fa6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth_0.png   ground_truth_2.png  ground_truth_6.png\r\n",
      "ground_truth_1.png   ground_truth_3.png  ground_truth_7.png\r\n",
      "ground_truth_10.png  ground_truth_4.png  ground_truth_8.png\r\n",
      "ground_truth_11.png  ground_truth_5.png  ground_truth_9.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls vis_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee74db41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('vis_outputs/ground_truth_0.png')\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7bb111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(2,64,3,padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU())\n",
    "                                    \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64,64,3,padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(64,64,3,padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(64,64,3,padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.layer5 = nn.Sequential(nn.Conv2d(64,128,3,padding=1),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU())        \n",
    "        self.layer6 = nn.Sequential(nn.Conv2d(128,128,3,padding=1),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.layer7 = nn.Sequential(nn.Conv2d(128,128,3,padding=1),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU())\n",
    "        self.layer8 = nn.Sequential(nn.Conv2d(128,128,3,padding=1),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU())\n",
    "        self.fc1 = nn.Linear(128*16*16,1024)\n",
    "        self.fc2 = nn.Linear(1024,8)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = out.reshape(-1,128* 16* 16)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f1664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a513a4a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/uig08207/DeepHomographyEstimation.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load saved weights to model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ckpt_pth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/uig08207/DeepHomographyEstimation.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_pth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/uig08207/DeepHomographyEstimation.pth'"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "\n",
    "# load saved weights to model\n",
    "ckpt_pth = '/home/uig08207/DeepHomographyEstimation.pth'\n",
    "ckpt = torch.load(ckpt_pth)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.005, momentum=0.9)\n",
    "optimizer.load_state_dict(ckpt['optimizer'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce71039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 05:50:56.738349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2023-06-27 05:50:57.069767: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0xbbe1470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of images loading:  1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 05:50:58.219410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TRYING ON IR DATASET\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from data_read import *\n",
    "from net import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "parser.add_argument('--dataset_name', action=\"store\", dest= \"dataset_name\",default=\"Faces\",help='MSCOCO,GoogleMap,GoogleEarth,DayNight')\n",
    "\n",
    "\n",
    "parser.add_argument('--epoch_load_one', action=\"store\", dest=\"epoch_load_one\", type=int, default=9,help='epoch_load_one')\n",
    "\n",
    "\n",
    "parser.add_argument('--epoch_load_two', action=\"store\", dest=\"epoch_load_two\", type=int, default=9,help='epoch_load_two')\n",
    "\n",
    "parser.add_argument('--epoch_load_three', action=\"store\", dest=\"epoch_load_three\", type=int, default=9,help='epoch_load_three')\n",
    "\n",
    "parser.add_argument('--num_iters', action=\"store\", dest=\"num_iters\", type=int, default=20,help='num_iters')\n",
    "\n",
    "parser.add_argument('--feature_map_type', action=\"store\", dest=\"feature_map_type\", default='special',help='regular or special')\n",
    "\n",
    "parser.add_argument('--initial_type', action=\"store\", dest=\"initial_type\", default='vanilla',help='vanilla, simple_net, multi_net')\n",
    "\n",
    "parser.add_argument('--load_epoch_simplenet', action=\"store\", dest=\"load_epoch_simplenet\", default=40,help='load_epoch_simplenet')\n",
    "\n",
    "parser.add_argument('--load_epoch_multinet', action=\"store\", dest=\"load_epoch_multinet\", default=[100,100,80],help='load_epoch_multinet')\n",
    "\n",
    "\n",
    "input_parameters = parser.parse_args([])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def construct_matrix_regression(batch_size,network_output,network_output_2=[0]):\n",
    "    extra=tf.ones((batch_size,1))\n",
    "    predicted_matrix=tf.concat([network_output,extra],axis=-1)\n",
    "    predicted_matrix=tf.reshape(predicted_matrix,[batch_size,3,3])\n",
    "    if len(np.shape(network_output_2))>1:\n",
    "        predicted_matrix_2=tf.concat([network_output_2,extra],axis=-1)\n",
    "        predicted_matrix_2=tf.reshape(predicted_matrix_2,[batch_size,3,3])\n",
    "    hh_matrix=[]\n",
    "    for i in range(batch_size):\n",
    "        if len(np.shape(network_output_2))>1:\n",
    "            hh_matrix.append(np.linalg.inv(np.dot(predicted_matrix_2[i,:,:],predicted_matrix[i,:,:])))\n",
    "        else:\n",
    "            hh_matrix.append(np.linalg.inv(predicted_matrix[i,:,:]))\n",
    "        #hh_matrix.append(predicted_matrix[i,:,:])\n",
    "    \n",
    "    #return tf.linalg.inv(predicted_matrix+0.0001)\n",
    "    return np.asarray(hh_matrix)\n",
    "\n",
    "def initial_motion_COCO():\n",
    "    # prepare source and target four points\n",
    "    matrix_list=[]\n",
    "    for i in range(1):\n",
    "       \n",
    "        src_points=[[0,0],[127,0],[127,127],[0,127]]\n",
    "\n",
    "        tgt_points=[[32,32],[160,32],[160,160],[32,160]]\n",
    "\n",
    "    \n",
    "        src_points=np.reshape(src_points,[4,1,2])\n",
    "        tgt_points=np.reshape(tgt_points,[4,1,2])\n",
    "\n",
    "        # find homography\n",
    "        h_matrix, status = cv2.findHomography(src_points, tgt_points,0)\n",
    "        matrix_list.append(h_matrix)\n",
    "    return np.asarray(matrix_list).astype(np.float32)\n",
    "\n",
    "\n",
    "def construct_matrix(initial_matrix,scale_factor,batch_size):\n",
    "    #scale_factor size_now/(size to get matrix)\n",
    "    initial_matrix=tf.cast(initial_matrix,dtype=tf.float32)\n",
    "    \n",
    "    scale_matrix=np.eye(3)*scale_factor\n",
    "    scale_matrix[2,2]=1.0\n",
    "    scale_matrix=tf.cast(scale_matrix,dtype=tf.float32)\n",
    "    scale_matrix_inverse=tf.linalg.inv(scale_matrix)\n",
    "\n",
    "    scale_matrix=tf.expand_dims(scale_matrix,axis=0)\n",
    "    scale_matrix=tf.tile(scale_matrix,[batch_size,1,1])\n",
    "\n",
    "    scale_matrix_inverse=tf.expand_dims(scale_matrix_inverse,axis=0)\n",
    "    scale_matrix_inverse=tf.tile(scale_matrix_inverse,[batch_size,1,1])\n",
    "\n",
    "    final_matrix=tf.matmul(tf.matmul(scale_matrix,initial_matrix),scale_matrix_inverse)\n",
    "    return final_matrix\n",
    "\n",
    "\n",
    "\n",
    "def average_cornner_error(batch_size,predicted_matrix,u_list,v_list,top_left_u=0,top_left_v=0,bottom_right_u=127,bottom_right_v=127):\n",
    "    \n",
    "    four_conner=[[top_left_u,top_left_v,1],[bottom_right_u,top_left_v,1],[bottom_right_u,bottom_right_v,1],[top_left_u,bottom_right_v,1]]\n",
    "    four_conner=np.asarray(four_conner)\n",
    "    four_conner=np.transpose(four_conner)\n",
    "    four_conner=np.expand_dims(four_conner,axis=0)\n",
    "    four_conner=np.tile(four_conner,[batch_size,1,1]).astype(np.float32)\n",
    "    \n",
    "    new_four_points=tf.matmul(predicted_matrix,four_conner)\n",
    "    \n",
    "    new_four_points_scale=new_four_points[:,2:,:]\n",
    "    new_four_points= new_four_points/new_four_points_scale\n",
    "    \n",
    "    \n",
    "    u_predict=new_four_points[:,0,:]\n",
    "    v_predict=new_four_points[:,1,:]\n",
    "    \n",
    "    average_conner=tf.reduce_mean(tf.sqrt(tf.math.pow(u_predict-u_list,2)+tf.math.pow(v_predict-v_list,2)))\n",
    "\n",
    "    \n",
    "    \n",
    "    return average_conner\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_feature_map(input_tensor):\n",
    "    bs,height,width,channel=tf.shape(input_tensor)\n",
    "    path_extracted=tf.image.extract_patches(input_tensor, sizes=(1,3,3,1), strides=(1,1,1,1), rates=(1,1,1,1), padding='SAME')\n",
    "    path_extracted=tf.reshape(path_extracted,(bs,height,width,channel,9))\n",
    "    path_extracted_mean=tf.math.reduce_mean(path_extracted,axis=3,keepdims=True)\n",
    "\n",
    "    #path_extracted_mean=tf.tile(path_extracted_mean,[1,1,1,channel,1])\n",
    "    path_extracted=path_extracted-path_extracted_mean\n",
    "    path_extracted_transpose=tf.transpose(path_extracted,(0,1,2,4,3))\n",
    "    variance_matrix=tf.matmul(path_extracted_transpose,path_extracted)\n",
    "    \n",
    "    tracevalue=tf.linalg.trace(variance_matrix)\n",
    "    row_sum=tf.reduce_sum(variance_matrix,axis=-1)\n",
    "    max_row_sum=tf.math.reduce_max(row_sum,axis=-1)\n",
    "    min_row_sum=tf.math.reduce_min(row_sum,axis=-1)\n",
    "    mimic_ratio=(max_row_sum+min_row_sum)/2.0/tracevalue\n",
    "    \n",
    "    return  tf.expand_dims(mimic_ratio,axis=-1)\n",
    "\n",
    "\n",
    "if input_parameters.feature_map_type=='regular':\n",
    "    load_path_one='./checkpoints/'+input_parameters.dataset_name+'/level_one_regular/'\n",
    "\n",
    "    load_path_two='./checkpoints/'+input_parameters.dataset_name+'/level_two_regular/'\n",
    "\n",
    "    load_path_three='./checkpoints/'+input_parameters.dataset_name+'/level_three_regular/'\n",
    "\n",
    "    level_one_input=ResNet_first_input(if_regular=True)\n",
    "    level_one_template=ResNet_first_template(if_regular=True)\n",
    "    level_two_input=ResNet_second_input(if_regular=True)\n",
    "    level_two_template=ResNet_second_template(if_regular=True)\n",
    "    level_three_input=ResNet_third_input(if_regular=True)\n",
    "    level_three_template=ResNet_third_template(if_regular=True)\n",
    "\n",
    "elif input_parameters.feature_map_type=='special':\n",
    "\n",
    "    load_path_one='./checkpoints/'+input_parameters.dataset_name+'/level_one/'\n",
    "\n",
    "    load_path_two='./checkpoints/'+input_parameters.dataset_name+'/level_two/'\n",
    "\n",
    "    load_path_three='./checkpoints/'+input_parameters.dataset_name+'/level_three/'\n",
    "\n",
    "    level_one_input=ResNet_first_input()\n",
    "    level_one_template=ResNet_first_template()\n",
    "    level_two_input=ResNet_second_input()\n",
    "    level_two_template=ResNet_second_template()\n",
    "    level_three_input=ResNet_third_input()\n",
    "    level_three_template=ResNet_third_template()\n",
    "\n",
    "\n",
    "level_one_input.load_weights(load_path_one + 'epoch_'+str(input_parameters.epoch_load_one)+\"input_full\")\n",
    "\n",
    "level_one_template.load_weights(load_path_one + 'epoch_'+str(input_parameters.epoch_load_one)+\"template_full\")\n",
    "\n",
    "level_two_input.load_weights(load_path_two + 'epoch_'+str(input_parameters.epoch_load_two)+\"input_full\")\n",
    "\n",
    "level_two_template.load_weights(load_path_two  + 'epoch_'+str(input_parameters.epoch_load_two)+\"template_full\")\n",
    "\n",
    "level_three_input.load_weights(load_path_three + 'epoch_'+str(input_parameters.epoch_load_three)+\"input_full\")\n",
    "\n",
    "level_three_template.load_weights(load_path_three  + 'epoch_'+str(input_parameters.epoch_load_three)+\"template_full\")\n",
    "\n",
    "\n",
    "if input_parameters.initial_type=='vanilla':\n",
    "    initial_matrix=initial_motion_COCO()\n",
    "    initial_matrix=construct_matrix(initial_matrix,scale_factor=0.25,batch_size=1)\n",
    "\n",
    "if input_parameters.initial_type=='simple_net':\n",
    "    save_path_regression='./checkpoints/'+input_parameters.dataset_name+'/regression_stage_1/'\n",
    "    regression_network=Net_first()\n",
    "    regression_network.load_weights(save_path_regression + 'epoch_'+str(input_parameters.load_epoch_simplenet))\n",
    "\n",
    "if input_parameters.initial_type=='multi_net':\n",
    "    save_path_one='./checkpoints/'+input_parameters.dataset_name+'/regression_stage_1/'\n",
    "    save_path_two='./checkpoints/'+input_parameters.dataset_name+'/regression_stage_2/'\n",
    "    save_path_three='./checkpoints/'+input_parameters.dataset_name+'/regression_stage_3/'\n",
    "    regression_network_one=Net_first()\n",
    "    regression_network_one.load_weights(save_path_one + 'epoch_'+str(input_parameters.load_epoch_multinet[0]))\n",
    "    regression_network_two=Net_second()\n",
    "    regression_network_two.load_weights(save_path_two + 'epoch_'+str(input_parameters.load_epoch_multinet[1]))\n",
    "    regression_network_three=Net_third()\n",
    "    regression_network_three.load_weights(save_path_three + 'epoch_'+str(input_parameters.load_epoch_multinet[2]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LK_layer_one=Lucas_Kanade_layer(batch_size=1,height_template=128,width_template=128,num_channels=1)\n",
    "\n",
    "LK_layer_two=Lucas_Kanade_layer(batch_size=1,height_template=64,width_template=64,num_channels=1)\n",
    "\n",
    "LK_layer_three=Lucas_Kanade_layer(batch_size=1,height_template=32,width_template=32,num_channels=1)\n",
    "\n",
    "\n",
    "LK_layer_regression=Lucas_Kanade_layer(batch_size=1,height_template=192,width_template=192,num_channels=3)\n",
    "\n",
    "\n",
    "if input_parameters.dataset_name=='MSCOCO':\n",
    "    data_loader_caller=data_loader_MSCOCO('val')\n",
    "\n",
    "if input_parameters.dataset_name=='GoogleMap':\n",
    "    data_loader_caller=data_loader_GoogleMap('val')\n",
    "\n",
    "if input_parameters.dataset_name=='GoogleEarth':\n",
    "    data_loader_caller=data_loader_GoogleEarth('val')\n",
    "\n",
    "if input_parameters.dataset_name=='DayNight':\n",
    "    data_loader_caller=data_loader_DayNight('val')\n",
    "\n",
    "if input_parameters.dataset_name=='Faces':\n",
    "    data_loader_caller = data_loader_Faces('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21dfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img, u_list, v_list, template_img = data_loader_caller.data_read_batch(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb38fa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# estimate init homography\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m test_input \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(\u001b[43minput_img\u001b[49m[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA)\n\u001b[1;32m      5\u001b[0m test_template \u001b[38;5;241m=\u001b[39m template_img[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m test_input \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(test_input, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_img' is not defined"
     ]
    }
   ],
   "source": [
    "# estimate init homography\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_input = cv2.resize(input_img[0], (128, 128), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "test_template = template_img[0].copy()\n",
    "test_input = cv2.cvtColor(test_input, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "data = np.dstack((test_input, test_template))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba00c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.expand_dims(data, axis=0)\n",
    "print(data.shape)\n",
    "data = torch.from_numpy(data)\n",
    "data = data.to(device)\n",
    "data = data.permute(0,3,1,2).float()\n",
    "outputs = model(data)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfe236",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs.cpu().detach().numpy()\n",
    "print(outputs)\n",
    "outputs = np.reshape(outputs, (4, 2))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "top_point    = (32,32)\n",
    "left_point   = (patch_size+32, 32)\n",
    "bottom_point = (patch_size+32, patch_size+32)\n",
    "right_point  = (32, patch_size+32)\n",
    "four_points = np.asarray([top_point, left_point, bottom_point, right_point], dtype=np.float32)\n",
    "homography_matrix = cv2.getPerspectiveTransform(four_points, np.add(four_points, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error=0.0    \n",
    "\n",
    "fk_loop=input_parameters.num_iters\n",
    "\n",
    "\n",
    "for iters in range(10000000):\n",
    "    input_img,u_list,v_list,template_img=data_loader_caller.data_read_batch(batch_size=1)\n",
    "    if len(np.shape(input_img))<2:\n",
    "        break\n",
    "\n",
    "   \n",
    "\n",
    "    if input_parameters.initial_type=='simple_net':\n",
    "#         input_img_grey=tf.image.rgb_to_grayscale(input_img)\n",
    "        input_img_grey=input_img\n",
    "        template_img_new=tf.image.pad_to_bounding_box(template_img, 32, 32, 192, 192)  \n",
    "#         template_img_grey=tf.image.rgb_to_grayscale(template_img_new)\n",
    "        template_img_grey=template_img_new\n",
    "        network_input=tf.concat([template_img_grey,input_img_grey],axis=-1)\n",
    "        homography_vector=regression_network.call(network_input,training=False)\n",
    "        extra=tf.ones((1,1))\n",
    "        initial_matrix=tf.concat([homography_vector,extra],axis=-1)\n",
    "        initial_matrix=tf.reshape(initial_matrix,[1,3,3])\n",
    "        initial_matrix=construct_matrix(initial_matrix,scale_factor=0.25,batch_size=1)\n",
    "\n",
    "    if input_parameters.initial_type=='multi_net':\n",
    "        input_img_grey=tf.image.rgb_to_grayscale(input_img)\n",
    "        template_img_new=tf.image.pad_to_bounding_box(template_img, 32, 32, 192, 192)  \n",
    "        template_img_grey=tf.image.rgb_to_grayscale(template_img_new)\n",
    "        network_input=tf.concat([template_img_grey,input_img_grey],axis=-1)\n",
    "        homography_vector_one=regression_network_one.call(network_input,training=False)\n",
    "        matrix_one=construct_matrix_regression(1,homography_vector_one)\n",
    "        template_img_new=LK_layer_regression.projective_inverse_warp(tf.dtypes.cast(template_img,tf.float32), matrix_one)\n",
    "        template_img_grey=tf.image.rgb_to_grayscale(template_img_new) \n",
    "        network_input=tf.concat([template_img_grey,input_img_grey],axis=-1)\n",
    "        homography_vector_two=regression_network_two.call(network_input,training=False)\n",
    "        matrix_two=construct_matrix_regression(1,homography_vector_one,homography_vector_two)\n",
    "        template_img_new=LK_layer_regression.projective_inverse_warp(tf.dtypes.cast(template_img,tf.float32), matrix_two)\n",
    "        template_img_grey=tf.image.rgb_to_grayscale(template_img_new)  \n",
    "        network_input=tf.concat([template_img_grey,input_img_grey],axis=-1)\n",
    "        homography_vector_three=regression_network_three.call(network_input,training=False)\n",
    "\n",
    "        extra=tf.ones((1,1))\n",
    "        initial_matrix=tf.concat([homography_vector_three,extra],axis=-1)\n",
    "        initial_matrix=tf.reshape(initial_matrix,[1,3,3])\n",
    "        initial_matrix=np.dot(initial_matrix[0,:,:], np.linalg.inv(matrix_two[0,:,:]))\n",
    "        initial_matrix=np.expand_dims(initial_matrix,axis=0)\n",
    "        initial_matrix=construct_matrix(initial_matrix,scale_factor=0.25,batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Create initial matrix from Keypoint detection and matching\n",
    "#     input_img_8bit = cv2.normalize(input_img[0][:,:,:3], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "#     template_img_8bit = cv2.normalize(template_img[0][:,:,:3], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "#     input_img_8bit = cv2.cvtColor(input_img_8bit, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "#     kp1, des1 = orb.detectAndCompute(input_img_8bit, None)\n",
    "#     kp2, des2 = orb.detectAndCompute(template_img_8bit, None)\n",
    "\n",
    "#     method = cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING\n",
    "#     matcher = cv2.DescriptorMatcher_create(method)\n",
    "#     matches = matcher.match(des1, des2, None)\n",
    "#     matches = sorted(matches, key=lambda x:x.distance)\n",
    "\n",
    "#     keep = int(len(matches) * keep_ratio)\n",
    "#     matches = matches[:keep]\n",
    "#     pts1 = np.zeros((len(matches), 2), dtype='float')\n",
    "#     pts2 = np.zeros((len(matches), 2), dtype='float')\n",
    "\n",
    "#     for (i, m) in enumerate(matches):\n",
    "#         pts1[i] = kp1[m.queryIdx].pt\n",
    "#         pts2[i] = kp2[m.trainIdx].pt\n",
    "\n",
    "#     H, mask = cv2.findHomography(pts1, pts2, method=cv2.RANSAC)\n",
    "#     initial_matrix = construct_matrix(H, scale_factor=0.25, batch_size=1)\n",
    "        \n",
    "    input_feature_one=level_one_input.call(input_img,training=False)\n",
    "    template_feature_one=level_one_template.call(template_img,training=False)\n",
    "\n",
    "    input_feature_two=level_two_input.call(input_feature_one,training=False)\n",
    "    template_feature_two=level_two_template.call(template_feature_one,training=False)\n",
    "\n",
    "    input_feature_three=level_three_input.call(input_feature_two,training=False)\n",
    "    template_feature_three=level_three_template.call(template_feature_two,training=False)\n",
    "\n",
    "\n",
    "    if input_parameters.feature_map_type=='regular':\n",
    "        input_feature_map_one=input_feature_one\n",
    "        template_feature_map_one=template_feature_one\n",
    "\n",
    "        input_feature_map_two=input_feature_two\n",
    "        template_feature_map_two=template_feature_two\n",
    "\n",
    "        input_feature_map_three=input_feature_three\n",
    "        template_feature_map_three=template_feature_three\n",
    "\n",
    "    elif input_parameters.feature_map_type=='special':\n",
    "                \n",
    "        input_feature_map_one=calculate_feature_map(input_feature_one)\n",
    "        template_feature_map_one=calculate_feature_map(template_feature_one)\n",
    "\n",
    "        input_feature_map_two=calculate_feature_map(input_feature_two)\n",
    "        template_feature_map_two=calculate_feature_map(template_feature_two)\n",
    "\n",
    "        input_feature_map_three=calculate_feature_map(input_feature_three)\n",
    "        template_feature_map_three=calculate_feature_map(template_feature_three)\n",
    "        \n",
    "        \n",
    "    updated_matrix=initial_matrix\n",
    "    for j in range(fk_loop):\n",
    "        try:\n",
    "            updated_matrix=LK_layer_three.update_matrix(template_feature_map_three,input_feature_map_three,updated_matrix)\n",
    "        except:\n",
    "            print ('s')\n",
    "\n",
    "    updated_matrix=construct_matrix(updated_matrix,scale_factor=2.0,batch_size=1)\n",
    "    for j in range(fk_loop):\n",
    "        try:\n",
    "            updated_matrix=LK_layer_two.update_matrix(template_feature_map_two,input_feature_map_two,updated_matrix)\n",
    "        except:\n",
    "            print ('s')\n",
    "            \n",
    "    updated_matrix=construct_matrix(updated_matrix,scale_factor=2.0,batch_size=1)\n",
    "    for j in range(fk_loop):\n",
    "        try:\n",
    "            updated_matrix=LK_layer_one.update_matrix(template_feature_map_one,input_feature_map_one,updated_matrix)\n",
    "        except:\n",
    "            print ('s')\n",
    "\n",
    "    predicted_matrix=updated_matrix\n",
    "    print(predicted_matrix)\n",
    "\n",
    "    cornner_error=average_cornner_error(1,predicted_matrix,u_list,v_list,top_left_u=0,top_left_v=0,bottom_right_u=127,bottom_right_v=127)\n",
    "\n",
    "    \n",
    "    print (np.float(cornner_error))\n",
    "    if iters>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ce9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_img[0,:,:,:])\n",
    "plt.show()\n",
    "plt.imshow(input_feature_map_one[0,:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(template_img[0,:,:,:])\n",
    "plt.show()\n",
    "plt.imshow(template_feature_map_one[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34255c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
